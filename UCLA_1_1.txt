nohup: ignoring input
pretrain?: False
hidden_size: 256
n_heads: 8
num_layers: 4
batch_size: 4
==================================================
[INFO] Starting NW-UCLA dataset processing on cuda...
==================================================
[INFO]: proability of dropping regularization: 0.1
[INFO]: data being repeated by 10 times
Collected 10200 sequences for train + val.
Each sequence shape: torch.Size([3, 64, 20, 1])
Collected 464 sequences for test.
Each sequence shape: torch.Size([3, 64, 20, 1])
[INFO] Number of classes: 10
====================================================================================================
====================================================================================================
====================================================================================================
====================================================================================================
pretrained model loaded successfully!
[INFO] finetuning the entire T1 model...
is T1 freezed? False
unfreezing layers: None
  0%|          | 0/95 [00:00<?, ?it/s]/home/peng.1007/BPMT/baseline/action_recognition/cascadeformer_1_1/joint/n_ucla/base_dataset.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  seq   = torch.tensor(self.seqs[idx],   dtype=torch.float32)
  1%|          | 1/95 [00:56<1:27:44, 56.01s/it]  2%|▏         | 2/95 [01:51<1:26:25, 55.76s/it]  3%|▎         | 3/95 [02:47<1:25:56, 56.05s/it]  4%|▍         | 4/95 [03:42<1:24:05, 55.44s/it]  5%|▌         | 5/95 [04:37<1:22:53, 55.26s/it]  6%|▋         | 6/95 [05:33<1:22:18, 55.49s/it]  7%|▋         | 7/95 [06:29<1:21:28, 55.55s/it]  8%|▊         | 8/95 [07:24<1:20:17, 55.37s/it]  9%|▉         | 9/95 [08:18<1:18:52, 55.03s/it] 11%|█         | 10/95 [09:12<1:17:36, 54.78s/it] 12%|█▏        | 11/95 [10:09<1:17:35, 55.42s/it]